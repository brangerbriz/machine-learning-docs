# Data is Key

Large amounts of quality data are key to the success of any machine learning algorithm. After all, models can only be as good as the data used to train them. The bottleneck in an ML pipeline (see [The ML Pipeline](the-ml-pipeline.html)) is often in access to large quantities of high-quality data. It is not uncommon for the majority of the development time it takes to implement an ML solution will be spent collecting, creating<span class="marginal-note" data-info="Don't limit your ML ideas based on the data you may have easily accessible. Remember that you can create and synthesis new datasets for the sole purpose of training."></span>, organizing, cleaning and preprocessing, your data (see [Normalization & Preprocessing](normalization-and-preprocessing.html)).

## Data Quantities

There are [so many factors at play](http://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html) in any given ML algorithm that it is often difficult to know the minimum amount of data needed for your model to start performing well in practice. Simple data statistics like standard deviation, number and quality of features, number of model parameter, and problem task all play a significant role in determining how much data is needed. That said, the answer to the question of "how much data do I need?" is usually always "more data."

I've seen rules of thumb that suggest that the minimum number of samples needed to train a model range anywhere from `10xN` to `N^2` where `N` is the number of features (columns) in your data. The truth is that the amount of data required scales with model capacity and complexity.
<span class="marginal-note" data-info="Model capacity is the number of functions that a model can technically approximate. The more weights/parameters a model has the higher it's capacity will be, and therefore the more unique functions it can learn to approximate. The problem is that as the ability to learn more functions increases, so does the likelyhood that the function learned during training will not be the true function that represents the underlying data trying to be learned. Therefore a model with too many parameters can lead to overfitting."></span> Like many things in machine learning, the quantity of data required to solve a given problem must be empirically tested.
<span class="marginal-note" data-info="The truth is that there is so much that we don't yet understand about ML. We are at just the tip of the iceberg when it comes to ML discovery and understanding. We can mathematically prove that neural networks with unlimited neurons can model any function (see [General Purpose Algorithms](general-purpose-algorithms.html)), but we don't yet know how to systematically train them effectively. Huge progress has been made in this area recently, but ultimately ML researchers are poking around in the dark to find answers. Often solutions or techniques work but there is very little explanation or understanding as to why. It is for this reason that training ML models is sometimes considered more of an art than a science. If you're not sure how something will work the best solution is probably just to try it."></span>

<!-- > Marginal note: "must be empirically tested". Cover info currently in empirical-testing.md -->

## Data Representation

The way that you represent your data is arguably more important than the amount of raw data itself. Data is represented using features, and the ones you choose (or better yet, [learn](https://en.wikipedia.org/wiki/Feature_learning)) influence how effectively your model can learn about the data (see [Features & Design Matrices](features-and-design-matrices.html)). Just because you have 20 columns in your database to represent a user doesn't mean that all 20 of those columns are necessary to solve a simple classification task. You may find that your model performs best using only 4 features from each data sample.

It's also very common not to use the features from your dataset directly, but rather process them first to compute new features that are more helpful for your model. For instance, if you were training a classifier to predict durations of metro-rail commutes given a database of card swipe timestamps, a ride duration feature (`swipe_out - swipe_in`) may be more beneficial than two separate swipe timestamp features. Knowing how best to represent your data is difficult and empirical testing is often the best way to determine what features to use or derive from your data when training your models.

## Training Data vs Test Data

There is a critically important distinction to be made between data used to train a model and data used to test the performance of that model (see [Performance Measures](performance-measures.html)). The same data **cannot** be used to both train your model and evaluate its performance, as this will lead to drastic overfitting.
<span class="marginal-note" data-info='Overfitting means that your model is memorizing your training data rather than learning to generalize, a quality needed to perform well on unseen data. The solution to overfitting is to use [regularization](regularization.html). The opposite of overfitting is underfitting. Underfitting occurs when model capacity is too small and the model is unable to learn patterns from the training data. The solution to underfitting is to increase model capacity, use more training data, and/or change your training data representation (see [Features and Design Matrices](features-and-design-matrices.html)).'></span>
The training of every supervised machine learning model requires that you split your data, using the majority of it for training and holding out the minority for testing and model evaluation. A split of 80% training data and 20% test data is common. If you have a lot of data, you can experiment with 85%+ training data. Model performance will likely always be better on your test data, but hopefully only a few percentage points away from the accuracy evaluation on your test data. Test performance is a measure of how well your model will generalize to unseen real-world data it will encounter "in the wild" once it is deployed. Data holdout is an important topic and I recommend checking out the [data split Wikipedia page](https://en.wikipedia.org/wiki/Training%2C_test%2C_and_validation_sets) for more info.

Finally, [Kaggle](https://kaggle.com) is an amazing data science community that hosts paid data science competitions and publishes publicly available datasets and code examples. Perusing the site should give you a good overview of the types of data representations that are often used in machine learning.

Next: [Machine Learning Models](machine-learning-models.html)<br>
Previous: [General Purpose Algorithms](general-purpose-algorithms.html)
