## Regularization

- meant to reducing model capacity
- Batch normalization
- Gradient clipping (vanishing/exploading gradients)
- Dropout
	- touch on ensemble learning
- Early stopping
	- yes this is a form of regularization, because you are reducing model capacity (in this case by limiting compute time)
- Anorthodox stuff
	- multiple objective learning (can't remember if that is the right name)