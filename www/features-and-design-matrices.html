<!DOCTYPE html>
<html>
<head><meta charset='utf-8'>
  <title>markdown/features-and-design-matrices.md</title>
  <link rel="stylesheet" href="css/github-markdown.css">
</head>
<body class='markdown-body'>
<h1 id="features-and-design-matrices">Features and Design Matrices</h1>
<p>Features are any attribute or measurable property that can be used to describes a data sample. With tabular data, features can be thought of as column names. For instance, in a database of forum users, the features that describe each user may include <code>user_id</code>, <code>username</code>, <code>email</code>, <code>date_joined</code>, <code>number_of_posts</code>, <code>number_of_comments</code>, <code>number_of_visits</code>, <code>time_on_site</code>, <code>last_seen</code>. Similarly, the Iris dataset describes observations of each wild Iris flower using five features: <code>iris_species</code>, <code>sepal_length</code>, <code>sepal_width</code>, <code>petal_length</code> and <code>petal_width</code>. In a traditional supervised learning task, a model can be trained to predict the <code>iris_species</code> from the four other features.</p>
<p>While it can help to think of features as the column names in a database of samples, the most important thing to realize about features are that they are any information that you use to describes your data. <strong>Features are the input to a machine learning model</strong>. The process by which you determine which features to include in the data that is fed to your model is called <em>feature selection</em>.</p>
<h2 id="raw-features">Raw Features</h2>
<p>It is often satisfactory to use the &quot;as is&quot; representation of your data (with <a href="normalization-and-preprocessing.html">normalization and preprocessing</a> of course) with your machine learning model successfully. For instance, when your data represents an image, the features are usually the raw pixel values of that image. When your data is audio, the features may be the raw audio samples, between <code>-1.0</code> and <code>1.0</code> sampled at 44.1Khz. </p>
<p>For categorical features a <a href="one-hot-encoding.html">one-hot encoding</a> transformation must be used, but it is common to keep these data as their raw labels.</p>
<h2 id="derived-features">Derived Features</h2>
<p>Sometimes it is helpful to augment your data representation with derived features before using it as input to an ML model. Think of derived features as a metadata of sorts, features about your data rather than the raw features themselves. For image data, perhaps you use a quantized version of the pixel brightness histogram instead of the raw image pixels themselves. For audio data, perhaps you use the fast-fourier transform of the audio data as features instead of the raw samples themselves. Raw features and derived features are not mutually exclusive, and it is perfectly fine to use both raw features and derived features at the same time. </p>
<p>The reason that derived features are helpful is that they provides your model with an initial understanding of the data that its fed in a way that may be helpful &quot;prior knowledge&quot; for the type of task it is solving. In theory, a neural nework with infinite resources and the correct training algorithm could learn to perfectly learn a set of neurons that equivalently match any derived feature given enough data. But in reality, if a pixel value histogram is important to the task at hand, it may be easier to provide that information to the model directly as a feature instead of tasking the model with learning to extract the relevant information itself using only the raw pixels.</p>
<p>Derived features should be used with some caution because they introduce a significant amount of developer bias into the <a href="the-ml-pipeline.html">machine learning pipeline</a>. Just because we <em>think</em> that a particular set of features would be useful to a model in learning to execute some task doesn&#39;t mean that those features <em>are actually</em> useful. A significant amount of recent research has suggested that <a href="feature-learning.html">feature learning</a> is often preferable to using hand-crafted or derived features because the model has the oppertunity to learn what kind of features to care about rather than being explicitly told by the programmer.</p>
<h2 id="what-makes-good-features-">What Makes Good Features?</h2>
<p>The best features provide the most information in the smallest amount of space and are linearly independent from other features to avoid repetition. </p>
<ul>
<li>talk about covariance (when covariance is positive they change together, when negative, they are inversely related). Zero covariance == independent vars.<ul>
<li>Show covariance matrix</li>
</ul>
</li>
</ul>
<h2 id="design-matrices">Design Matrices</h2>
<p>Features are the input data</p>
<p>Machine learning algorithms must operate on numerical data. </p>
<ul>
<li><p>What are features?</p>
<ul>
<li>Iris dataset</li>
<li>Pixels in an image (mnist)</li>
<li>Feature = Input</li>
</ul>
</li>
<li><p>Features are often hand chosen, especially in traditional machine learning pipelines. However there is much contemporary research that encourages against hand-selected features. Biases are introduced based on assumptions of what is (and is not) useful to the learning system. But results are often better if the human doesn&#39;t make the choice for the model, but rather the model decides for itself what is and is-not helpful to know about.</p>
</li>
<li><p>Feature Learning</p>
</li>
<li><p>Features should have low co-variance.</p>
<ul>
<li>Use some co-variance matrices as an example</li>
</ul>
</li>
</ul>
</body>
</html>
