<!DOCTYPE html>
<html lang="en">
    <head>
        <!--
              +++++++++++++
            +++++++++++++++++
          +++++++++++++++++++++
         +++++++ ---------- ++++       ____                                         ____       _
        ++++++++|  ______  |+++++     |  _ \                                       |  _ \     (_)
        ++++++__| |______| |+++++     | |_) |_ __ __ _ _ __   __ _  ___ _ __       | |_) |_ __ _ ____
        +++++|  _________  |+++++     |  _ <| '__/ _` | '_ \ / _` |/ _ \ '__|      |  _ <| '__| |_  /
        +++++| |_________| |+++++     | |_) | | | (_| | | | | (_| |  __/ |         | |_) | |  | |/ /
         ++++|_____________|++++      |____/|_|  \__,_|_| |_|\__, |\___|_| _______ |____/|_|  |_/___|
          +++++++++++++++++++++                              __ | |       |_______|
            +++++++++++++++++                                \___/
              +++++++++++++
        -->
        <meta charset="utf-8">
        <meta http-equiv="x-ua-compatible" content="ie=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <title> Branger_Briz </title>

        <link rel="icon" type="image/png" href="images/bb.svg">
        <meta name="description" content="we are a full­service digital agency+lab made up of artists, strategists, educators && programmers bent on articulating contemporary culture. we produce award winning work for brands, agencies, and cultural institutions around the world.">

        <!-- for Google+ -->
        <meta itemprop="name" content="Branger_Briz">
        <meta itemprop="description" content="we are a full­service digital agency+lab made up of artists, strategists, educators && programmers bent on articulating contemporary culture">
        <meta itemprop="image" content="images/bb.svg">
        <!-- for Twitter -->
        <meta name="twitter:card" content="summary_large_image">
        <meta name="twitter:site" content="@branger_briz">
        <meta name="twitter:title" content="Branger_Briz">
        <meta name="twitter:description" content="we are a full­service digital agency+lab made up of artists, strategists, educators && programmers bent on articulating contemporary culture.">
        <meta name="twitter:creator" content="@branger_briz">
        <!-- Twitter summary card with large image must be at least 280x150px -->
        <meta name="twitter:image:src" content="images/bb.svg">
        <!-- for Facebook -->
        <meta property="og:title" content="Branger_Briz">
        <meta property="og:type" content="article">
        <meta property="og:url" content="http://brangerbriz.com/">
        <meta property="og:image" content="images/bb.svg">
        <meta property="og:description" content="we are a full­service digital agency+lab made up of artists, strategists, educators && programmers bent on articulating contemporary culture.">
        <meta property="og:site_name" content="Branger_Briz">

        <!-- CSS -->
        <!-- consider including normalize.css -->
        <link rel="stylesheet" href="css/bb-fonts.css">
        <link rel="stylesheet" href="css/bb-styles.css">
        <link rel="stylesheet" href="css/bb-responsive.css"><!-- optional media-queries -->
        <link rel="stylesheet" href="css/bb-code-colors.css"><!-- optional syntax highlighting -->
        <link rel="stylesheet" href="css/bb-animations.css"><!-- optional intro animations -->

    </head>
    <body>
        <section id="logo"></section>
<!-- The content below this line is injected using `marked` in `build.sh` -->
<h1 id="twitterbot-part-4-transfer-learning-fine-tuning-and-user-personalization">Twitterbot Part 4: Transfer Learning, Fine-tuning, and User Personalization</h1>
<p>In <a href="twitterbot-part-3-model-inference-and-deployment.html">Part 3</a> of this tutorial we used seven million tweets to train a base model in Keras, which we then deployed in a browser environment using Tensorflow.js. In this chapter, we&#39;ll learn how we can use a technique called <em>transfer learning</em> to fine-tune our base model using tweets from individual twitter accounts. We&#39;ll create a graphical application that allows you to train models using an individual user&#39;s tweets and use them to generate synthetic tweets in the style of that user.</p>
<h2 id="individual-twitter-user-data">Individual Twitter User Data</h2>
<p>Until now, we&#39;ve been using twitter data aggregated from hundreds of thousands of different twitter accounts. This has worked well for the purpose of training a base model to synthesize general tweets, but now we&#39;d like to imitate individual twitter user accounts. To do so, we&#39;ll use the Twitter API. We&#39;ll create a Node.js server<span class="marginal-note" data-info="We'll be recreating the code from this [tweet-server](https://github.com/brangerbriz/tweet-server) repository, if you care to jump ahead."></span> that we can use to download a user&#39;s public tweets given their username. We&#39;ll then use this server process to download twitter data at will upon request from our training code.</p>
<pre class="code">
    <code class="bash" data-wrap="false">
# leave the tfjs-tweet-generation directory to create and enter tweet-server/
cd ..
mkdir tweet-server
cd tweet-server/
    </code>
</pre>

<p>Next create a <code>package.json</code> file inside of <code>tweet-server/</code> and populate it using the contents below.</p>
<pre class="code">
    <code class="json" data-wrap="false">
{
  "name": "tweet-server",
  "version": "0.1.0",
  "description": "Download twitter data using an HTTP REST API.",
  "main": "server.js",
  "scripts": {
    "start": "node server.js"
  },
  "author": "Brannon Dorsey + Nick Briz",
  "license": "GPL-3.0",
  "dependencies": {
    "cors": "^2.8.4",
    "express": "^4.16.3",
    "twit": "^2.2.11"
  }
}
    </code>
</pre>

<p>Install the project dependencies using NPM.</p>
<pre class="code">
    <code class="bash" data-wrap="false">
npm install
    </code>
</pre>

<p>Before you can use the Twitter API, you have to create a Twitter API application using a developer account at <a href="https://developer.twitter.com" target="_blank">developer.twitter.com</a>. You&#39;ll need to submit an application to become a developer as well as to create a new application.<span class="marginal-note" data-info="Both applications are instantly approved in my experience, but in theory, the process can take longer."></span> Once that&#39;s done you&#39;ll need to generate consumer API keys and access tokens.</p>
<section class="media" data-fullwidth="false">
    <img src="images/twitter-api.png"> 
</section>

<p>We&#39;ll use Express to create our own REST API using Node.js. We&#39;ll download a user&#39;s tweets using GET requests by providing a twitter username in the URL. A request like <code>http://localhost:3000/api/barackobama</code> will return a JSON object containing Obama&#39;s tweets.</p>
<p>Create a new file called <code>server.js</code> and fill it with the contents below. Replace the <code>TWITTER_*</code> constants using values from your Twitter API app.</p>
<pre class="code">
    <code class="javascript" data-wrap="false">
const path = require('path')
const cors = require('cors')
const Twit = require('twit')
const express = require('express')
const app = express()
const http = require('http').Server(app)

// populate these constants using the API keys from your Twitter API app
const TWITTER_CONSUMER_KEY=""
const TWITTER_CONSUMER_SECRET=""
const TWITTER_ACCESS_TOKEN=""
const TWITTER_ACCESS_TOKEN_SECRET=""

// create an instance of Twit, which we'll use to access the twitter API
const T = new Twit({
    // goto: https://apps.twitter.com/ for keys
    consumer_key: TWITTER_CONSUMER_KEY,
    consumer_secret: TWITTER_CONSUMER_SECRET,
    access_token: TWITTER_ACCESS_TOKEN,
    access_token_secret: TWITTER_ACCESS_TOKEN_SECRET,
    timeout: 60 * 1000 // optional HTTP request timeout for requests
})

// allows Cross-Origin Resource Sharing (CORS) on the /api endpoint
app.use('/api', cors())

// all GET requests to /api/ should include a user value in the path. 
// This user value will be interpreted as twitter handle.
app.get('/api/:user', async (req, res) => {
    const user = req.params.user
    try {
        const tweets = await getUserTweets(user)
        console.log(`[server] /api/:user got tweets for user ${user}`)
        res.json({ error: null, tweets: tweets })
    } catch (err) {
        console.log(`[server] /api/:user got tweets for user ${user} with error:`)
        console.error(err)

        let message = `Error fetching tweets for user ${user}`
        if (err.statusCode) {
            res.status(err.statusCode)
        } else {
            res.status(500)
        }

        if (err.message) message = err.message
        res.json({ error: message, tweets: null })
    }
})

http.listen(3000, () => { 
    console.log('[server] Listening on http://0.0.0.0:3000')
})

// download ~3,200 of the user's most recent tweets.
async function getUserTweets(user) {
    // https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html

    const tweets = []
    let batch = await getUserTweetBatch(user)

    tweets.push(...batch)
    console.log(`[twitter] Got ${batch.length} new tweets. Total ${tweets.length}`)
    // the twitter API we're using only allows tweets to be downloaded in groups
    // of 200 per request, so we create a loop to download tweets in batches
    while (batch.length > 1) {
        // use tweet ids for pagination
        let id = batch[batch.length - 1].id
        batch = await getUserTweetBatch(user, id)
        tweets.push(...batch)
        console.log(`[twitter] Got ${batch.length} new tweets. Total ${tweets.length}`)
    }
    // discard metadata and only return the contents of the tweets
    return tweets.map(tweet => tweet.text)
}

// download a batch of 200 tweets using maxId for pagination
function getUserTweetBatch(user, maxId) {
    return new Promise((resolve, reject) => {
        T.get('statuses/user_timeline', {
            screen_name: user,
            count: 200, // max (but can be called again with max_id)
            max_id: maxId,
            include_rts: true
        }, (err, data, res) => {
            if (err) reject(err)
            else resolve(data)
        })
    })
}
    </code>
</pre>

<p>In this script, we register an HTTP route for all <code>GET /api/:user</code> requests, where <code>:user</code> is a twitter handle. All requests that match this route will trigger a download of ~3,200 tweets using the <a href="https://www.npmjs.com/package/twit" target="_blank">twit npm package</a> via <code>getUserTweets()</code>. This function downloads 200 tweets at a time in a loop using <code>getUserTweetBatch()</code>. Batches of tweets are combined into one array and stripped of metadata; the value returned from <code>getUserTweets()</code> is an array of tweet contents only. If the download succeeded a JSON object containing the tweets is returned as the result of the HTTP request: <code>{ error: null, tweets: [...] })</code>. If there was an error downloading tweets, the tweets object is null: <code>{ &quot;error&quot;: &quot;Sorry, that page does not exist.&quot;, &quot;tweets&quot;: null }</code>.</p>
<pre class="code">
    <code class="bash" data-wrap="false">
node server.js
    </code>
</pre>

<p>Let&#39;s query our new API in another terminal window. </p>
<pre class="code">
    <code class="bash" data-wrap="false">
# query the tweet-server using curl. This should take a few seconds and then
# print Obama's tweets as a JSON object like the one below.
curl http://localhost:3000/api/barackobama
    </code>
</pre>
<pre class="code">
    <code class="json" data-wrap="false">
{
    "error":null,
    "tweets": [ 
        "Today, I’m proud to endorse even more Democratic candidates who aren’t just running against something, but for some… https://t.co/oqewS0Y8vZ",
        "From civil servants to organizers, the young people I met in Amsterdam today are doing the hard work of change. And… https://t.co/mlAp2SRZlP",
        "The antidote to government by a powerful few is government by the organized, energized many. This National Voter Re… https://t.co/3W5pfaUdKd",
        "The first class of Obama Fellows is full of leaders like Keith—hardworking, innovative, and dedicated to partnering… https://t.co/nOd6FzH23n",
        "We will always remember everyone we lost on 9/11, thank the first responders who keep us safe, and honor all who de… https://t.co/ku270JQnwl",
        "RT @nowthisnews: If you still don't think the midterms will affect you, @BarackObama is back to spell out just how important they are https…",
        "Today I’m at the University of Illinois to deliver a simple message to young people all over the country: You need… https://t.co/brM6Vd7j2R",
        "Yesterday I met with high school students on Chicago’s Southwest side who spent the summer learning to code some pr… https://t.co/hY9B0mSQB9",
        "Congratulations to Hawaii for winning the Little League World Series! You make America very proud.",
        ...
    ]
}
    </code>
</pre>

<p>Over in the terminal running our <code>server.js</code> process, you should see logs from the <code>curl</code> query. If something went wrong, it will likely appear here.</p>
<pre class="code">
    <code class="plain" data-wrap="false">
[server] Listening on http://0.0.0.0:3000
[twitter] Got 200 new tweets. Total 200
[twitter] Got 200 new tweets. Total 400
[twitter] Got 200 new tweets. Total 600
[twitter] Got 200 new tweets. Total 800
[twitter] Got 200 new tweets. Total 1000
[twitter] Got 200 new tweets. Total 1200
[twitter] Got 200 new tweets. Total 1400
[twitter] Got 200 new tweets. Total 1600
[twitter] Got 200 new tweets. Total 1800
[twitter] Got 200 new tweets. Total 2000
[twitter] Got 200 new tweets. Total 2200
[twitter] Got 200 new tweets. Total 2400
[twitter] Got 199 new tweets. Total 2599
[twitter] Got 199 new tweets. Total 2798
[twitter] Got 198 new tweets. Total 2996
[twitter] Got 200 new tweets. Total 3196
[twitter] Got 36 new tweets. Total 3232
[twitter] Got 1 new tweets. Total 3233
[server] /api/:user got tweets for user barackobama
    </code>
</pre>

<p>We&#39;ll use this server to download tweets later in the tutorial, so leave it running. In the meantime, let&#39;s talk a bit about <em>transfer learning</em>.</p>
<h2 id="transfer-learning">Transfer Learning</h2>
<p>Transfer learning is the process of using knowledge gained from one task to solve another task. In practice, this technique involves re-using model weights that were pre-trained using a large dataset as the initial weights of a new model trained using a smaller dataset. In non-transfer learning scenarios model weights are initialized using a random distribution. With transfer learning, a new model&#39;s weights are initialized using a checkpoint from a model that was trained using a different dataset, loss function, and/or performance metric.</p>
<p>The intuition behind transfer learning is that knowledge gained from one task can be transferred, through shared model weights, to a different but related task. In a character-level text generation task, our model must learn to extract language patterns entirely from scratch using the training data. Our untrained RNN model has no conception of the english language. Before it can learn to string related words together to form realistic looking sentences, it must learn to combine the right characters to create words at all. If the training data is too small, it&#39;s likely that our model won&#39;t even be able to generate english looking text, let alone anything that looks like a tweet.</p>
<p>Twitter&#39;s API restricts tweet downloads to a mere <a href="https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-user_timeline.html" target="_blank">3,200 tweets per user account</a>, which isn&#39;t much data at all. If we were to train a model with randomly initialized weights using only tweets from a single user&#39;s account as training data, the model would perform very poorly. I would expect the model to either not be able to extract useful language patterns from such little text, or to instead memorize the training data and output only exact samples found in the training set. Instead of training our individual twitter user models using randomly initialized weights, we will instead initialize them using the weights of our base model, which we trained using over seven million tweets in <a href="twitterbot-part-3-model-inference-and-deployment.html">Part 3</a>. Our base model has already learned to create english words and sentences, the appropriate lengths of tweets, and how to RT, @mention, and #hashtag. This prior knowledge extends our capability to train a model to imitate an individual twitter user using very little training data.</p>
<p>Here&#39;s a pseudo-code example of how the model fine-tuning process works using transfer learning.</p>
<pre class="code">
    <code class="javascript" data-wrap="false">
// train a new model for a very long time on a very large dataset
const baseModel = train(createModel(), 'large-dataset.txt')

// fine-tune the pre-trained model using a small dataset
const fineTunedModel = train(baseModel, 'small-dataset.txt')
    </code>
</pre>

<h2 id="twitter-application">Twitter Application</h2>
<p>We&#39;ll create one last folder inside of the <code>twitterbot-tutorial/</code> directory we&#39;ve been working from since Part 1.</p>
<pre class="code">
    <code class="bash" data-wrap="false">
# leave the tweet-server directory to create and enter twitter-transfer-learning/
cd ..
mkdir twitter-transfer-learning
cd twitter-transfer-learning/
    </code>
</pre>

<p><code>twitter-transfer-learning/</code> will house our code for the rest of the tutorial. Here we&#39;ll create a small web application<span class="marginal-note" data-info="We'll be re-creating the web application from [this repo](https://github.com/brangerbriz/twitter-transfer-learning), which you can use for reference."></span> that:</p>
<ul>
<li>Loads our base model</li>
<li>Downloads twitter data using <code>tweet-server</code></li>
<li>Fine-tunes copies of our base model using twitter data from user accounts</li>
<li>Saves and loads our fine-tuned models</li>
<li>Generates tweets using any of our trained models</li>
<li>Provide a minimal user interface for accomplishing all of these tasks</li>
</ul>
<p>Let&#39;s start off by creating a boilerplate directory structure and downloading a few dependencies that we&#39;ll use later on.</p>
<pre class="code">
    <code class="bash" data-wrap="true">
# src/ for source code files, lib/ for third party libraries, and checkpoints/
# to save our base model.
mkdir src lib checkpoints

# download our utility functions, which are nearly identical to the ones we 
# created in Part 3.
wget -O src/utils.js https://raw.githubusercontent.com/brangerbriz/twitter-transfer-learning/master/src/utils.js

# download BBElements, a set of html/css/js components used for styling and
# branding @ Branger_Briz. This will make our app look pretty ;)
git clone https://github.com/brangerbriz/BBElements

# copy our base model to the new directory
cp -r ../tfjs-tweet-generation/checkpoints/base-model checkpoints/base-model
    </code>
</pre>

<p>Create a <code>package.json</code> file with the contents below.</p>
<pre class="code">
    <code class="json" data-wrap="false">
{
  "name": "twitter-transfer-learning",
  "version": "1.0.0",
  "scripts": {
    "start": "electron src/electron.js"
  },
  "author": "Brannon Dorsey <bdorsey@brangerbriz.com>",
  "dependencies": {
    "@tensorflow/tfjs-node": "^0.1.17",
    "@tensorflow/tfjs-node-gpu": "^0.1.17",
    "electron": "^2.0.8",
    "hyperparameters": "^0.25.5",
    "json2csv": "^4.2.1",
    "node-fetch": "^2.2.0"
}
    </code>
</pre>

<p>Install these dependencies using NPM.</p>
<pre class="code">
    <code class="bash" data-wrap="false">
npm install
    </code>
</pre>

<h3 id="basic-fine-tuning">Basic Fine-Tuning</h3>
<p>Let&#39;s create a new script called <code>src/fine-tune.js</code>, which we&#39;ll use to explore the process of downloading twitter data and fine-tuning our base model using transfer learning. This script will be a self-contained Node.js process where we&#39;ll demonstrate the fine-tuning process, void of UI code. We&#39;ll run this script from the command-line.</p>
<pre class="code">
    <code class="javascript" data-wrap="false">
#!/usr/bin/env node
const tf = require('@tensorflow/tfjs')
const fs = require('fs')
const path = require('path')
const utils = require('../src/utils')

// exit if the twitter-user parameter isn't included
if (process.argv[2] == null) {
    console.error(`usage: ${path.basename(process.argv[1])} twitter-user`)
    process.exit(1)
}

// try and load tfjs-node-gpu, but fallback to tfjs-node if no CUDA
require('@tensorflow/tfjs-node-gpu')
if (['webgl', 'cpu'].includes(tf.getBackend())) {
    require('@tensorflow/tfjs-node')
    console.log('GPU environment not found, loaded @tensorflow/tfjs-node')
} else {
    console.log('loaded @tensorflow/tfjs-node-gpu')
}
console.log(`using tfjs backend "${tf.getBackend()}"`)

// remove the leading @ character if it exists
const TWITTER_USER = process.argv[2].replace(/^@/, '')
const TWEET_SERVER = 'http://localhost:3000'

const BATCH_SIZE = 64
const SEQ_LEN = 64
const DROPOUT = 0.0
const OPTIMIZER = 'adam'
const FINETUNE_EPOCHS = 10
const VAL_SPLIT = 0.2

async function main() {

    console.log(`fetching tweets for user @${TWITTER_USER}`)
    let text, data
    try {
        [text, data] = await utils.loadTwitterData(TWITTER_USER, TWEET_SERVER)
    } catch(err) {
        console.error('Error downloading tweets.')
        if (err.message) console.error(err.message)
        process.exit(1)
    }
    console.log('download complete.')

    // these options will be reused between several of the utility functions
    const options = {
        batchSize: BATCH_SIZE,
        seqLen: SEQ_LEN,
        dropout: DROPOUT,
        oneHotLabels: true
    }

    const valSplitIndex = Math.floor(data.length * VAL_SPLIT)
    const valGenerator = utils.batchGenerator(data.slice(0, valSplitIndex), 
                                              options)
    const trainGenerator = utils.batchGenerator(data.slice(valSplitIndex), 
                                                options)

    const modelPath = 'file://' + path.resolve(__dirname, 
                                               '..', 
                                               'checkpoints', 
                                               'base-model', 
                                               'tfjs', 
                                               'model.json')
    let model = await tf.loadModel(modelPath)
    // update the model architecture to use the BATCH_SIZE and SEQ_LEN
    // we've chosen for the fine-tune process.
    model = utils.updateModelArchitecture(model, options)
    model.trainable = true
    model.compile({ optimizer: OPTIMIZER, loss: 'categoricalCrossentropy' })

    // Fine-tune the model using transfer learning
    await utils.fineTuneModel(model, 
                              FINETUNE_EPOCHS, 
                              BATCH_SIZE, 
                              trainGenerator, 
                              valGenerator)

    // save the model in checkpoints/TWITTER_USER
    const saveDir = path.resolve(__dirname, '..', 'checkpoints', TWITTER_USER)
    if(!fs.existsSync(saveDir)) fs.mkdirSync(saveDir)
    await model.save(`file://${ path.join(saveDir, 'tfjs') }`)

    // we'll update the model architecture one more time, this time for
    // inference. We set both the BATCH_SIZE and SEQ_LEN to 1 and make
    // the model weights untrainable.
    let inferenceModel = utils.updateModelArchitecture(model)
    model.trainable = false

    // Generate 2048 characters using the fine-tuned model.
    const seed = "This is a seed sentence."
    const generated = await utils.generateText(inferenceModel, seed, 2048, 5)
    console.log(generated)
}

main().catch(console.error)
    </code>
</pre>

<p>This script begins with a few dependency imports before checking if a command-line argument is defined with <code>process.argv[2] == null</code>. If it isn&#39;t, the program prints its usage and exits with an error code. If an argument was included it is interpreted as the <code>TWITTER_USER</code> later in the program. After this validation check, we <code>require(&#39;@tensorflow/tfjs-node-gpu&#39;)</code> and then check the value of <code>tf.getBackend()</code>. If your computer has an NVIDIA graphics card and CUDA installed<span class="marginal-note" data-info="See [ML Development Environment](ml-development-environment.html)"></span> the backend should now be &quot;tensorflow&quot;. If not, it will instead be either &quot;cpu&quot; or &quot;webgl&quot;, in which case we fallback to the non-GPU-accelerated version of tfjs-node with <code>require(&#39;@tensorflow/tfjs-node&#39;)</code>.</p>
<p>We define several global constants in this script for Twitter download settings and hyperparameters. If the user specified the <code>twitter-user</code> command line argument with an &quot;@&quot; character (e.g. &quot;<a href="https://twitter.com/branger_briz" target="_blank">@branger_briz</a>&quot;) we remove it. We also define the URL for an instance of <code>tweet-server</code> we wrote earlier in this chapter via <code>const TWEET_SERVER = &#39;http://localhost:3000&#39;</code>, before defining the hyperparameter values we&#39;ll use to fine-tune our model.<span class="marginal-note" data-info="These values were chosen via a hyperparameter search just like we did in [Part 3](twitterbot-part-3-model-inference-and-deployment.html), this time using data from an individual user's twitter account and weight initialization using the base model. This search was written in JavaScript and you can download the script from [here](https://github.com/brangerbriz/twitter-transfer-learning/blob/master/bin/hyperparameter-search.js)."></span> With this setup complete, we launch the <code>main()</code> function and log any errors to the console.</p>
<p>The <code>main()</code> function begins by downloading and encoding twitter data via <code>utils.loadTwitterData(TWITTER_USER, TWEET_SERVER)</code>. This function makes an HTTP request to our <code>tweet-server</code>&#39;s API and returns the JSON results or throws an error if something went wrong. Here&#39;s a peek at its source code inside <code>src/utils.py</code>:</p>
<pre class="code">
    <code class="javascript" data-wrap="false">
/**
 * Load data using a tweet-server (https://github.com/brangerbriz/tweet-server)
 * @function loadTwitterData
 * @param  {string} user A twitter user to load tweets for
 * @param  {string} tweetServer A url pointing to a tweet-server instance
 * @returns {Promise}
 * @throws TypeError
 */
async function loadTwitterData(user, tweetServer) {
    const response = await fetch(`${tweetServer}/api/${user}`)
    if (response.ok) {
        const json = await response.json()
        if (json.tweets) {
            const text = json.tweets.join('\n')
            const encoded = encodeText(text)
            return [text, encoded]
        }
    }
    throw TypeError(`Failed to load tweets for ${user}`)
}
    </code>
</pre>

<p>Once our data has been downloaded and encoded we create <code>valGenerator</code> and <code>trainGenerator</code> using <code>utils.batchGenerator()</code> just like we did in Python. <a href="https://github.com/brangerbriz/twitter-transfer-learning/blob/d5b238cb6e55090781554ed851ec17c1d5cfc898/src/utils.js#L80" target="_blank">This function</a> is a JavaScript rewrite of our <code>utils.io_batch_generator()</code> Python function without the lazy loading functionality. We&#39;ve removed this from our JavaScript implementation because we are fine-tuning our models using only ~3,200 tweets instead of 7,000,000+ and can fit all of this data into memory at once.</p>
<p>After loading our data generators we load our base model from disk and update its architecture to support our new values for <code>BATCH_SIZE</code>, <code>SEQ_LEN</code>, and <code>DROPOUT</code> with <code>utils.updateModelArchitecture()</code><span class="marginal-note" data-info="This function is a renamed version of buildInferenceModel() from generate.js in [Part 3](twitterbot-part-3-model-inference-and-deployment.html)."></span> We then further train our base model with <code>await utils.fineTuneModel()</code>. This function should look somewhat familiar to our Python train function with a few changes.</p>
<pre class="code">
    <code class="javascript" data-wrap="false">
// utils.fineTuneModel(...)
async function fineTuneModel(model, 
                             numEpochs, 
                             batchSize, 
                             trainGenerator, 
                             valGenerator, 
                             callbacks) {

    // keep a losses object to return at the end of fine-tuning
    const losses = {
        loss: [],
        valLoss: []
    }
    // reset the model's internal RNN states from wherever they were left
    // during the most recent model training
    model.resetStates()

    let lastEpoch = 0
    if (callbacks && typeof callbacks.onEpochBegin === 'function') {
        // if an onEpochBegin() callback was included, fire it now
        callbacks.onEpochBegin()
    }

    // Train epochs in an infinite loop
    while (true) {
        const [x, y, epoch] = trainGenerator.next().value
        const history = await model.fit(x, y, {
            batchSize: batchSize,
            epochs: 1,
            shuffle: false,
            yieldEvery: 'batch'
        })

        if (lastEpoch !== epoch) {
            const [x, y] = valGenerator.next().value
            console.log('evaluating model')
            const eval = await model.evaluate(x, y, { batchSize: batchSize })
            const valLoss = (await eval.data())[0]
            const loss = history.history.loss[0]
            let msg = `Epoch ${epoch} Train loss: ${loss} Val loss: ${valLoss}`
            console.log(msg)
            losses.loss.push(loss)
            losses.valLoss.push(valLoss)
            // Don't forget to reset states on each epoch! 
            model.resetStates()
            lastEpoch = epoch

            // Free the tensor memory
            x.dispose()
            y.dispose()

            // Call the onEpochEnd() and onEpochBegin() callbacks if they
            // were included as arguments

            if (callbacks && typeof callbacks.onEpochEnd === 'function') {
                callbacks.onEpochEnd(lastEpoch, loss, valLoss)
            }

            if (epoch != numEpochs && callbacks && 
                typeof callbacks.onEpochBegin === 'function') {
                callbacks.onEpochBegin()
            }
        }

        // Once we've trained for numEpochs, release the tensor memory and
        // return the losses object
        if (epoch == numEpochs) {
            x.dispose()
            y.dispose()
            return losses
        }
    }
}
    </code>
</pre>

<p>Once this function returns, we save our model using the Twitter user&#39;s account name in <code>checkpoints/</code>. Finally, we generate new tweets using our fine-tuned model by updating its architecture once again to accept <code>BATCH_SIZE</code> and <code>SEQ_LEN</code> values of <code>1</code> and calling <code>utils.generateText()</code>.<span class="marginal-note" data-info="This utility function is identical to the generateText() function from generate.js in [Part 3](twitterbot-part-3-model-inference-and-deployment.html)."></span></p>
<p>Let&#39;s try it out! Make sure your tweet server from earlier in the chapter is still running. If it&#39;s not, open a new terminal in <code>tweet-server</code> and run <code>node server</code>. In a separate terminal window, run the <code>src/fine-tune.js</code> script.</p>
<pre class="code">
    <code class="bash" data-wrap="false">
node src/fine-tune.js barackobama
    </code>
</pre>

<pre class="code">
    <code class="plain" data-wrap="true">
"In the middle class, in the carbon pollution share." President Obama on the Senate #AmericaLeads
It's ticket. That would be more than ever happened tonight after a beautiful private-section. #GetCovered #ImmigrationAction
"We cant take the power of the fight this year." President Obama #SOTU
"The first two years since the end of sears of three months, and subject on this." President Obama #ActOnClimate
"We have to make sure you can stand up to the planet." President Obama
The United States on Americans we can't stand a power of anniversary of the facts of the country talking about the #SOTU. https://t.co/nuniGBr0li
"With this deal with my family to health care reform: https://t.co/lAvZGRe7Ey http://t.co/0ynm9d80rg
RT @WhiteHouse: "A record-breaking this was about taxpayers will be here in Chicago tonight, the presidential is here: http://t.co/gzud9ssiV3 #GetCovered #DoYourJob https://t.co/jnz9hN0lVY
Today is a remarken to make change about the success of the minimum wage. http://t.co/k8sDgRE9Q7
RT @WhiteHouse: "We've got a stand against a fair hearing and an even about it." President Obama #SOTU
It's so easy to take a sector of middle-class families and want to go to start the planet for my features, but that made me want to see what we had all their consumers. #ACAWorks
"It's why we cant move our energy for constant of the capacity." President Obama #ItsOnUs
Its so fund." @OFA: President Obama says to #ActOnClimate about how to help close the private-sector job gain in America: http://t.co/ycydjKuYEm #ActOnClimate
    </code>
</pre>

<p>That&#39;s it! I find it amazing how powerful transfer learning can be. We fit our base model to Obama&#39;s tweets in only ten epochs using very limited training data, and yet, the generated text actually sounds like the former president.</p>
<h3 id="building-a-gui-application-with-electron">Building a GUI Application with Electron</h3>
<p>Now that we&#39;ve got the basics of model fine-tuning down using <code>bin/fine-tune.js</code>, we&#39;re going to create an electron application that let&#39;s us generate twitter bots using a graphical user interface.</p>
<section class="media" data-fullwidth="false">
    <img src="images/twitter-bot-generator-electron.png" alt="A screenshot of the final application.">
</section>

<!-- The content above this line is injected using `marked` in `build.sh` -->    
        <p>Return to the <a href="index.html">main page</a>.</p>

        <script src="js/BBElements/highlightJS/highlight.pack.js"></script>
       <script src="js/BBElements/BBElements.js"></script>
    </body>
</html>